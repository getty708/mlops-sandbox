# Ref: https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_2-improving_resource_utilization/model_repository/text_recognition/config.pbtxt
name: "text_recognition"
backend: "onnxruntime"
max_batch_size : 8
input [
  {
    name: "input.1"
    data_type: TYPE_FP32
    dims: [ 1, 32, 100 ]
  }
]
output [
  {
    name: "308"
    data_type: TYPE_FP32
    dims: [ 26, 37 ]
  }
]

# [Eexperiment 2] Enable dynamic batching
# dynamic_batching { }

instance_group [
    {
      # [Experiment 1] No dynamic batching and no model parallelism
      count: 1
      # [Experiment 3] Added to enable model parallelism
      # count: 2
      kind: KIND_CPU
    }
]